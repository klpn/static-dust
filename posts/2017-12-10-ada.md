---
tags: datavetenskap
title: Programmera till framtiden
author: Karl Pettersson
lang: sv
---

[För två år sedan](http://klpn.se/2015/12/10/datorlos-utvecklare-2/) skrev jag
om 200-årsminnet av Ada Lovelaces födelse. Hon har kallats världens första
programmerare, på grund av de algoritmer hon utarbetade för Charles Babbages
*Analytical Engine*. Vissa ansåg att detta jubileum var något större än
Nobeldagen. Även om det i år inte är något jämnt jubileum tänker jag hellre på
minnet av hennes insatser än på spektaklen kring Nobelprisen.

En väsentlig sak som skiljer Lovelace från en typisk utvecklare idag är att
hon aldrig fick testköra de programrutiner hon utvecklade. Babbages analytiska
maskin var en hårdvarudesign som aldrig resulterade i någon verklig hårdvara:
det var inte möjligt med de resurser som stod till buds på 1800-talet. Först
omkring 1945 kom de första elektroniska datorerna.

Denna observation av hur datorutvecklingen kan bromsas genom att hårdvarans och
mjukvarans utveckling kommer i otakt kan sättas i relation till dagens
diskussion om framtidsutsikterna för artificiell intelligens överlägsen
människans primitiva kognition. Vissa har hävdat att vi är på väg mot en
"teknologisk singularitet", där sådana övermänskliga artificiella intelligenser
förbättrar sig själva och skapar ännu mer övermänskliga intelligenser, som gör
det samma i en allt snabbare takt. En del har även försökt sig på att förutsäga
tidpunkt för denna singularitet: Ray Kurzweil har argumenterat för att den
kommer att inträffa 2045. Sådana förutsägelser bygger till en del på
extrapolering från olika varianter av "Moores lag", som går ut på att det under
lång tid skett en exponentiell ökning av datorernas beräkningskapacitet, i
förhållande till t.ex.\ pris eller storlek.

En invändning mot detta resonemang som t.ex.\ framförts av @stephenson04, med hänvisning
till @lanier00, är att även om hårdvaran blir snabbare och snabbare, gäller
fortfarande att "software is shit", och hårdvara utan användbar mjukvara är
inte mycket mer än en komplicerad värmeanläggning. Med andra ord står vi inför ett
problem (eller en räddning, om singulariteten, som många befarat, skulle
innebära katastrof för mänskligheten) motsatt Lovelaces och Babbages. De hade
inte tillgång till hårdvara för att köra sin mjukvara, men vi vi kommer inte att ha
tillgång till mjukvara som kan utnyttja möjligheterna i vår hårdvara.

Jo, vi har som sagt haft digitala datorer i ungefär 70 års tid, och det verkar i
alla fall som om de främsta innovationerna inom mjukvara gjorts under första
halvan av denna period. Det som hänt efter 1980, och gjort att datorerna
förändrat vårt vardagsliv i många avseenden, är i huvudsak att billigare och
smidigare hårdvara möjliggjort vidare spridning av tekniken.

Högnivåspråk från 1957--58, som Fortran och Lisp, används fortfarande för
utveckling idag, även om det förstås skett en del moderniseringar av dessa.
Operativsystem utvecklades fortfarande i assembler under 1960-talet, som med
den första versionen av Unix från 1969, men 1973 kom C. @lanier00 påstår sig ha
hatat Unix på 1970-talet och säger att han inte skulle ha satsat på
datavetenskap om han fått veta att detta system skulle vara i ropet 2000. Fortfarande
2017 ligger Unix och C till grund för en stor del av vår IT-infrastruktur.
Servrar tillgängliga via Internet och så kallade superdatorer, som används för
komplexa beräkningar, drivs idag mestadels av någon variant av Linux, och
Android har en Linuxkärna. Apples system, macOS, iOS och andra, är också de
Unix i botten. Alla dessa system utvecklas fortfarande till stor del i C. Det
har under de senaste åren förvisso utvecklats Unixliknande system i nyare
språk, som Rust [@redox]; det återstår att se om dessa system kommer att få
någon större spridning. Grafiska användargränssnitt, som har haft stor
betydelse för datorernas tillgänglighet, saknades i de tidiga versionerna av
Unix men utvecklades av Xerox på 1970-talet, fast den första kommersiella
hårdvaran som kunde köra dessa system kostade 16\ 500 dollar 1981
[@wiki:xeroxstar], vilket kanske skulle motsvara ca 500\ 000 kronor idag.

Windows är sedan 1990-talet dominerande operativsystem på persondatorer. Det
har också till stor del utvecklats i C. Dock är det inte ett Unixsystem i
grunden. I början kördes det som ett program under MS-DOS, ett system anpassat
för 1980-talets PC-hårdvara, utan funktioner som minnesskydd och säkerhet för
flera användare, som system för större datorer haft sedan 1960-talet. Det var
först 2001, med Windows XP, som dessa funktioner stöddes fullt ut av de
Microsoftsystem som levererades förinstallerade på vanliga persondatorer.
Windows NT hade haft sådan funktionalitet sedan 1993, men det var för tungt för
en dåtida standard-PC. Nu verkar trenden vara att Microsoft gör Windows mer
Unixlikt, därför att de annars inte skulle klara konkurrensen från Linux och
macOS: Windows 10 har subsystem för Linux [@wsldocs], till vilket kommer
PowerShell med fri licens [@psgit] och OpenSSH för Windows [@winsshgit].

I går såg jag Ulf Malmros film *Flykten till framtiden* på TV. Svante, en svårt
hjärtsjuk ung man som lever 1973 hittar ett t-banetåg, som tar honom till 2016,
där han träffar en nutida ung flicka, Elsa, varefter de åker fram och tillbaka
mellan de båda årtalen för att manipulera historien så att hans situation kan
lösas. Han blir förstås initialt förvirrad när han kommer till 2016 och stöter
på folk med iPhone, men han lär sig ganska snabbt och uppfattar det inte som
något trolleri: konceptet är inte *så* svårt att förstå utifrån den teknologi
som fanns allmänt tillgänglig 1973. När Elsa kommer tillbaka till 1973 ser hon
å andra sidan en löpsedel med Jan Guillou och suckar "har han alltid funnits".
Det går också att tänka sig en film där hon arbetat som webbutvecklare mot
LAMP-servrar 2016, åkt tillbaka till 1970-talet och stött ihop med någon som
loggat in på ett Unixsystem med någon terminal. "Har det alltid funnits?"

## Referenser
